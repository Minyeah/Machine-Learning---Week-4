---
title: "Practical Machine Learning Week 4 Project - Mike Bacas"
author: "Michael Bacas"
date: "January 28, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview
The goal of this project is to use data from accelerometers on the belt, forearm, arm, 
and dumbell to determine how well a participant was performing barbell lifts. To provide 
input information, six participants were asked to perform barbell lifts correctly and 
incorrectly in 5 different ways. I will create a model to try to predict how well
each partcipant was executing his or her lifts using a model and various figures.

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Load the needed packages
```{r results='hide'}
library(caret)
library(corrgram)
```

## Import and clean the needed files
First I will import the data and learn the basic characteristics of the data. I want to see which variables and observations will
be useful for prediction and which data must be removed to make the data usable.

```{r}
lifts.train = read.csv('pml-training.csv')
lifts.test  = read.csv('pml-testing.csv')
print(dim(lifts.train))
print(dim(lifts.test))
```

# Exclude aggregated rows from the training set (rows with new_window = yes)
Remove aggregated rows from the data as this does not help me in prediction.

```{r}
lifts.train = lifts.train[lifts.train$new_window == 'no', ]
```

# Columns to be removed: metadata columns ('X' refers to the first unnamed column)
Remove meta data columns that will not be useful.

```{r}
md_columns = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2',
               'cvtd_timestamp', 'new_window', 'num_window')
lifts.train = lifts.train[ , !(names(lifts.train) %in% md_columns)]
```

# Columns to be removed: no values in the the testing data set
Remove empty and "n/a" columns that will not add anything to the prediction model.

```{r}
lifts.test.nona <- lifts.test[ , colSums(is.na(lifts.test)) < nrow(lifts.test) ]
names.nona.variables <- names(lifts.test.nona)
names.nona.variables <- names.nona.variables[(names.nona.variables %in% names(lifts.train))]
names.nona <- append(names.nona.variables, 'classe')

lifts.train = lifts.train[names.nona]
```

## Are there any correlated variables in the dataset? 
If so, we can simplify the dataset further by identifying pairs of strongly correlated variales.
```{r}
lifts.train.sample = lifts.train[sample(nrow(lifts.train), 500), ]
corrgram(lifts.train.sample, order = TRUE, lower.panel=panel.pie)

```

## Big multi-scatter plot into a PDF
This chart provides the dependcies between individual parits of variables and the classe variable. The output is available in the 
"multi-scatter-pmd.pdf" file in the GitHub repository.

```{r}
pdf("multi-scatter-pmd", 50, 50)
pairs(lifts.train.sample, pch = 21, bg = c("red", "green3", "blue", "yellow", "green")[unclass(lifts.train.sample$classe)])
dev.off()
```

## Run the prediction models
The random forest model will be trained. I will test a version with all 53 remaining variables as well as a version with feature space reduced by PCA.

```{r results='hide'}
fitRfCv <- train(classe ~ ., data = lifts.train, ntree = 100, method = 'rf', trControl = trainControl(method = "cv", number = 5))
fitRfPcaCv <- train(classe ~ ., data = lifts.train, ntree = 100, method = 'rf', preProcess = "pca", trControl = trainControl(method = "cv", number = 5))
```

After the training I display the results and confusion matrices of both prediction models.
```{r}
print(fitRfCv$finalModel)
print(fitRfPcaCv$finalModel)
```
The model with all features performs better than the one with PCA pre-processing.

## Prediction
```{r}
lifts.prediction = predict(fitRfCv, lifts.test)

print(lifts.prediction)
```
